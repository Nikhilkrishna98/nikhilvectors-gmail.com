{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**.What are deep learning modelling techniques to handle textual data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deep learning modelling techniques to handle textual data are:CNN and RNN which utilises techniques like 1)word2vec 2)tokenization 3)word embedding 4)Padding Sequence etc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**.What is RNN?Why we use them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks(RNN) are a type of neural network that uses the output from the previous step which is fed as the input to the current step. RNN's are mainly used for the process of sequence classification,sentiment classification and video classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**.State down few problems in RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  two main problems in RNN are 1)Vanishing Gradient problem and 2)Gradient Explosion\n",
    "\n",
    "Other than these issues thereare few other like:\n",
    "    1)It cannot process very long sequences as it ues tanh as activation function.\n",
    "    2)It stands out as an unstable model when Relu function is used.\n",
    "    3)RNN's cannot be stacked into very deep models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**.What is vanishing gradient and gradient explosion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanishing gradient problem arises during the training of artificial neural network by using grdient based methods like backpropagation. In this issue the gradient will be vanishingly small that results in no change of weight from its previous value.At worst case it may even stop neural network training completely.\n",
    "\n",
    "Gradient explosion problems occur when the large error gradients accumulate and result in a very large updates to nerual network model weights during training.When the magnitudes of gradients accumulate it might result in an unstable network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**.What is LSTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short Term Memory(LSTM) is a variation of Recurrent Neural Network. It was intoduced in order to avoid the vanishing gradient problem and it can be trained with sequences of sentences which it can retain for a longer period of time,It consists of three gates which are forget gate,input gate and output gate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**.Benefits of LSTM over RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main benefit of using LSTM over RNN is it can avoid the vanishing gradient problem because of it's control gte present in it and also it can be trained with long sequences of dependencies as it can retain memory which is not possible in RNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
