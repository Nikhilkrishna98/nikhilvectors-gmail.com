{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** Why we need to convert words into vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion of words into words is done in order to make the deep learning algorithms ingest the words and process on it and also helps to make a much better understanding of the Natural Language Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** What are the differences in between CBOW (Continuous Bag-of-Words) and Skip-gram?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Skip-gram the centre word is taken and a window of context or neighbor words are used to predict the next context word out to some window size for each centre word,In CBOW the opposite takes place where the centre word is predicted by summing the vectors of surrounding words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.** Is CBOW and BOW (Bag-of-words) are the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CBOW and BOW are not he same,the BOW is an approach for dealing with words and context in the text processing without retaining the words order whereas in CBOW the focus word is taken and the the other words in the line with respect to the word and it's position is considered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** What are the use cases of CBOW and Skip-gram?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CBOW and Skip-gram are the two architectures of Word2Vec.It entirely depends on the type of data and word frequency that are being used.CBOW are generally used for the purpose of text generation and Skip-gram is used in the text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.**  Do you know any alternatives of Word2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alternatives to Word2Vec are Glove,Wang2Vec,Context2Vec and ELMo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6.** Define Word Embedding in your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embedding is a technique used in Natural Language Processing where words or phrases are converted into vectors of real numbers which involves embedding of the words. Word Embeddings are the distributed representations of the texts in  an n-dimensional space. They are highly recommended for solving nlp problems. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
