{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** What do you mean by transfer learning in NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Learning is the technique of transferring the knowledge trained on one model to another model in order to train it. Transfer Learning is now used in most of NLP models like Embeddings from Language MOdels(ELMO) and Bidirectional Encoder Representations from Transfer(BERT).Transfer Learning are of 4 types they are:1)Domain Specific 2)Cross Lingual Learning 3)Multi-task Learning and 4)Sequential Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.**Write name of 5 pretrained models famous in the field of NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5 pretrained models used in nlp are\n",
    "1)Bidirectional Encoder Representations from Transfer(BERT)\n",
    "2)Robustly Optimized BERT Pretraining Approach(RoBERTa)\n",
    "3)GPT-3\n",
    "4)ALBERT\n",
    "5)XLNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.**Diffrence betwwen auto encoding and auto regressive?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto Encoding models rely on the encoder part of the transformers and does not use maask which helps the model to look all the tokens in the attention heads.The corrupted version of the setences are used as the inputs for pretraining which are obtained by masking the tokens and the original sentences are the target.BERT is an auto encoding model.\n",
    "Auto Regressive models rely on the decoder part of the transformers and uses an attention mask so that the models can look only the tokens before in the attention heads.GPT-2 is a commonly used auto regressive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.**What do you mean by masked attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masked Attention is used in the attention based models in order to prevent the attention mechanism of a transformer to getting know about the token that is going to be present at the next position of a sentence beforetraining so that the model can attain robust state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.**What is BERT used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional Encoder Representations from Transformers are used for wide variety of language tasks like Sentiment Analysis, Classification tasks, Question Answering Tasks, Named Entity Recognition etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6.**What is diffrence between GPT2 and BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT uses the Transformer Encoder blocks and Self Attention mechanism while the GPT-2 uses the Transformer Decoder blocks and also the Masked Self Attention mechanism."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
